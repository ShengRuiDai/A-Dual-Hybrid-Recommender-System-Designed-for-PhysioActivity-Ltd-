{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c124c9",
   "metadata": {},
   "source": [
    "Combining content filtering and collaborative filtering scores for hybrid scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860588f5",
   "metadata": {},
   "source": [
    "Outputs Top 10 rankings for each user based on hybrid score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6d9482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>trainer_id</th>\n",
       "      <th>content_score</th>\n",
       "      <th>sim_score</th>\n",
       "      <th>hybrid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>U0000</td>\n",
       "      <td>444</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.512319</td>\n",
       "      <td>0.541874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>U0000</td>\n",
       "      <td>70</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>U0000</td>\n",
       "      <td>394</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.194508</td>\n",
       "      <td>0.382968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>U0000</td>\n",
       "      <td>471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.726540</td>\n",
       "      <td>0.363270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>U0000</td>\n",
       "      <td>609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715403</td>\n",
       "      <td>0.357702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  trainer_id  content_score  sim_score  hybrid_score\n",
       "61   U0000         444       0.571429   0.512319      0.541874\n",
       "13   U0000          70       0.857143   0.000000      0.428571\n",
       "51   U0000         394       0.571429   0.194508      0.382968\n",
       "66   U0000         471       0.000000   0.726540      0.363270\n",
       "76   U0000         609       0.000000   0.715403      0.357702"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_path = 'hybrid.xlsx'\n",
    "content_df = pd.read_excel(excel_path, sheet_name='Content')\n",
    "cf_df = pd.read_excel(excel_path, sheet_name='Item-CF')\n",
    "\n",
    "def guess_field(df, like):\n",
    "    candidates = [c for c in df.columns if like.lower() in c.lower()]\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    return df.columns[0]\n",
    "\n",
    "user_field = guess_field(content_df, 'user')\n",
    "trainer_field = guess_field(content_df, 'trainer')\n",
    "content_score_field = guess_field(content_df, 'score')\n",
    "cf_score_field = guess_field(cf_df, 'score')\n",
    "\n",
    "content_df = content_df.rename(columns={user_field:'user_id', trainer_field:'trainer_id', content_score_field:'content_score'})\n",
    "cf_df = cf_df.rename(columns={user_field:'user_id', trainer_field:'trainer_id', cf_score_field:'sim_score'})\n",
    "\n",
    "merge_df = pd.merge(\n",
    "    content_df, cf_df,\n",
    "    on=['user_id', 'trainer_id'],\n",
    "    how='outer'\n",
    ")\n",
    "merge_df['content_score'] = merge_df['content_score'].fillna(0)\n",
    "merge_df['sim_score'] = merge_df['sim_score'].fillna(0)\n",
    "\n",
    "# Weighting for a mixed fraction (equal weighted average)\n",
    "merge_df['hybrid_score'] = (merge_df['content_score'] + merge_df['sim_score']) / 2\n",
    "\n",
    "# Grouped by user, top 10 per user\n",
    "topn_df = merge_df.sort_values(['user_id', 'hybrid_score'], ascending=[True, False]).groupby('user_id').head(10)\n",
    "\n",
    "topn_df.to_excel('user_top10_hybrid.xlsx', index=False)\n",
    "\n",
    "topn_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b1820",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f591d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "behavior_enhanced = pd.read_excel('user_top10_hybrid.xlsx', sheet_name='Evaluation')\n",
    "trainer_reco_df = pd.read_excel('user_top10_hybrid.xlsx', sheet_name='hybridTop10')\n",
    "\n",
    "# Construction of ideal set (click/watch behaviour as positive samples)\n",
    "user_ideals = (\n",
    "    behavior_enhanced[behavior_enhanced['action_type'].isin(['watch', 'click'])]\n",
    "    .groupby('user_id')['trainer_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Ideal set sample distribution\n",
    "ideal_lens = [len(v) for v in user_ideals.values()]\n",
    "ideal_stats = {\n",
    "    'mean': np.mean(ideal_lens),\n",
    "    'median': np.median(ideal_lens),\n",
    "    'min': np.min(ideal_lens),\n",
    "    'max': np.max(ideal_lens)\n",
    "}\n",
    "\n",
    "# 4. indicators（Precision@10, Recall@10, NDCG@10, MAP@10）\n",
    "precision_list, recall_list, ndcg_list, ap_list = [], [], [], []\n",
    "\n",
    "def dcg_score(recommended, ideal_set):\n",
    "    return sum((1/np.log2(idx+2)) for idx, tid in enumerate(recommended) if tid in ideal_set)\n",
    "\n",
    "def ndcg_score(recommended, ideal_set, k):\n",
    "    idcg = sum(1/np.log2(i+2) for i in range(min(len(ideal_set), k)))\n",
    "    dcg = dcg_score(recommended, ideal_set)\n",
    "    return dcg / idcg if dcg > 0 else 0\n",
    "\n",
    "for user_id, ideals in user_ideals.items():\n",
    "    recos = trainer_reco_df[trainer_reco_df['user_id'] == user_id].sort_values('hybrid_score', ascending=False)['trainer_id'].tolist()\n",
    "    if not recos or not ideals:\n",
    "        continue\n",
    "    hits = [tid for tid in recos if tid in ideals]\n",
    "    precision = len(hits) / len(recos)\n",
    "    recall = len(hits) / len(ideals) if ideals else 0\n",
    "    ndcg = ndcg_score(recos, ideals, k=len(recos))\n",
    "    # MAP\n",
    "    num_hits, sum_precisions = 0, 0\n",
    "    for idx, tid in enumerate(recos):\n",
    "        if tid in ideals:\n",
    "            num_hits += 1\n",
    "            sum_precisions += num_hits / (idx + 1)\n",
    "    ap = sum_precisions / min(len(ideals), len(recos)) if ideals else 0\n",
    "\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    ndcg_list.append(ndcg)\n",
    "    ap_list.append(ap)\n",
    "\n",
    "precision_at10 = np.mean(precision_list)\n",
    "recall_at10 = np.mean(recall_list)\n",
    "ndcg_at10 = np.mean(ndcg_list)\n",
    "map_at10 = np.mean(ap_list)\n",
    "\n",
    "# Novelty\n",
    "novelty_scores = []\n",
    "for uid in trainer_reco_df['user_id'].unique():\n",
    "    rec_trainers = set(trainer_reco_df[trainer_reco_df['user_id']==uid]['trainer_id'])\n",
    "    hist_trainers = user_ideals.get(uid, set())\n",
    "    novelty = len(rec_trainers - hist_trainers) / len(rec_trainers) if rec_trainers else 0\n",
    "    novelty_scores.append(novelty)\n",
    "novelty_stats = {\n",
    "    'mean': np.mean(novelty_scores),\n",
    "    'median': np.median(novelty_scores)\n",
    "}\n",
    "\n",
    "# Coverage\n",
    "user_coverage = trainer_reco_df['user_id'].nunique() / behavior_enhanced['user_id'].nunique()\n",
    "item_coverage = trainer_reco_df['trainer_id'].nunique() / behavior_enhanced['trainer_id'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9491a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ideal set sample distribution': {'mean': 56.718,\n",
       "  'median': 57.0,\n",
       "  'min': 34,\n",
       "  'max': 79},\n",
       " 'Precision@10': 0.2097,\n",
       " 'Recall@10': 0.03747740388550982,\n",
       " 'NDCG@10': 0.20619795825271678,\n",
       " 'MAP@10': 0.0825072619047619,\n",
       " 'novelty': {'mean': 0.7903000000000001, 'median': 0.8},\n",
       " 'User Coverage': 1.0,\n",
       " 'Trainer Coverage': 0.816}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_trainer_enhanced = {\n",
    "    'Ideal set sample distribution': ideal_stats,\n",
    "    'Precision@10': precision_at10,\n",
    "    'Recall@10': recall_at10,\n",
    "    'NDCG@10': ndcg_at10,\n",
    "    'MAP@10': map_at10,\n",
    "    'novelty': novelty_stats,\n",
    "    'User Coverage': user_coverage,\n",
    "    'Trainer Coverage': item_coverage\n",
    "}\n",
    "\n",
    "\n",
    "display(results_trainer_enhanced)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3644ec9d",
   "metadata": {},
   "source": [
    "1. Ideal Set Sample Distribution\n",
    "Average number of interested trainers per user: 56.7 (median: 57, minimum: 34, maximum: 79)\n",
    "\n",
    "The distribution of user behaviors is reasonable, diverse, and broadly covered."
   ]
  },
  {
   "cell_type": "raw",
   "id": "018b65a2",
   "metadata": {},
   "source": [
    "2. Accuracy Metrics\n",
    "\n",
    "Metric\tValue\tExplanation\n",
    "Precision@10\t0.210\tTop 10 hit rate is 21.0%, at a mid-to-high industry level\n",
    "Recall@10\t0.037\tTop 10 covers 3.7% of each user's interest set; reasonable for cold-start\n",
    "NDCG@10\t0.206\tGood ranking quality; list ordering metric meets expectations\n",
    "MAP@10\t0.083\tAverage precision of recommendation lists is significantly above baseline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9000f57b",
   "metadata": {},
   "source": [
    "3. Novelty\n",
    "Average novelty: 0.79 (median: 0.8)\n",
    "\n",
    "Among the recommended trainers, about 80% are \"trainers not previously interacted with\" by the user, reflecting a high degree of content exploration capability."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cdccb01",
   "metadata": {},
   "source": [
    "4. Coverage\n",
    "User Coverage: 1.0 (All users receive Top 10 recommendations)\n",
    "\n",
    "Trainer Coverage: 0.816 (At least 81.6% of trainers are recommended to at least one user; trainer exposure rate is extremely high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b2a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af1200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3ad90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
